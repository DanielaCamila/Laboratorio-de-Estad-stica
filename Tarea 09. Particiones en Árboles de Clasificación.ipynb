{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e5b4008-7a80-4e8a-bd2f-3d6c98b3781a",
   "metadata": {},
   "source": [
    "## T09 - Particiones en árboles de clasificación\n",
    "- Camila Daniela Zapata Castañeda\n",
    "- 547624\n",
    "- 23 de Abril del 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b0c1d9-b1bb-4ef4-806b-e44cbcbfb42b",
   "metadata": {},
   "source": [
    "**GINI:**\n",
    "- El índice de GINI es una medida que evalúa qué tan mezclados o impuros están los datos en un nodo. Mientras más mezcla de clases haya, más alto será el valor de GINI. Este valor se usa luego en la función de costo, que ayuda al árbol a decidir dónde hacer la mejor partición. La idea es buscar el GINI más bajo, lo que indica que los datos están más homogéneos o \"puros\" en cada grupo después de la división.\n",
    "\n",
    "$$ \n",
    "\\text{Gini} = 1 - \\Sigma^k_{i=1} p_i^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Gini}{split} = \\frac{n\\text{izq}}{n} \\text{Gini}{\\text{izq}} + \\frac{n\\text{der}}{n} \\text{Gini}_{\\text{der}}\n",
    "$$\n",
    "\n",
    "**Entropía:**\n",
    "- La entropía mide el grado de desorden o incertidumbre en un conjunto de datos. Al igual que con el índice Gini, mientras más mezcladas estén las clases, más alto será el valor de la entropía. Este valor se usa en la función de costos, lo que permite al árbol elegir la división que reduzca más la incertidumbre. Una entropía alta significa que las clases están muy mezcladas, no hay una claramente dominante, y que se necesita más información para tomar una buena decisión.\n",
    "\n",
    "$$\n",
    "\\text{Entropía} = - \\Sigma^k_{i=1} p_i log_2(p_i)\n",
    "$$\n",
    "\n",
    "**Log Loss:**\n",
    "- Mide qué tan buenas son las probabilidades que predice tu modelo. A diferencia de métricas como la precisión, Log Loss no solo considera si acertaste o no, sino también con qué confianza hiciste la predicción. No se usa para decidir dónde dividir (como el Gini o la entropía), sino que evalúa el rendimiento general del modelo de clasificación. En esencia, mide el castigo que recibe el modelo por hacer predicciones incorrectas o poco seguras. El objetivo es obtener un Log Loss lo más bajo posible, lo cual indica que tu modelo está haciendo predicciones precisas y con alta seguridad.\n",
    "\n",
    "$$\n",
    "\\text{LogLoss} = -(y \\text{log}(p) + (1-y)\\text{log}(1-p))\n",
    "$$\n",
    "$$\n",
    "\\text{LogLoss} = -\\Sigma^k_{i=1} y_i \\text{log}(p_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75fe5543-74d6-4306-8df8-68543cb5d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6b122e-a0a8-4d97-bf37-c5323ca47e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos y split de train y test\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b50cfc-a839-415d-9744-dcf19aaec8b3",
   "metadata": {},
   "source": [
    "##### Arbol con GINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568de689-233c-47f9-97c1-1f213f367912",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "tree_gini.fit(X_train, y_train)\n",
    "y_pred_gini = tree_gini.predict(X_test)\n",
    "y_proba_gini = tree_gini.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a5f57d-d88b-45f5-a9e0-6fdc9a36de95",
   "metadata": {},
   "source": [
    "##### Arbol con Entropía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015cc662-844e-4b05-a1a0-389d4492fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "tree_entropy.fit(X_train, y_train)\n",
    "y_pred_entropy = tree_entropy.predict(X_test)\n",
    "y_proba_entropy = tree_entropy.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a045f-8687-4920-8aba-bc7607a6c5e2",
   "metadata": {},
   "source": [
    "##### Impresión de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e44dd11-2ac6-4c3d-aac6-1ae59860ae02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Árbol con GINI ==\n",
      "Precisión: 0.9333333333333333\n",
      "Log Loss: 2.402910225941144\n",
      "\n",
      "== Árbol con ENTROPÍA ==\n",
      "Precisión: 0.8888888888888888\n",
      "Log Loss: 4.004850376568572\n"
     ]
    }
   ],
   "source": [
    "print(\"== Árbol con GINI ==\")\n",
    "print(\"Precisión:\", accuracy_score(y_test, y_pred_gini))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_proba_gini))\n",
    "\n",
    "print(\"\\n== Árbol con ENTROPÍA ==\")\n",
    "print(\"Precisión:\", accuracy_score(y_test, y_pred_entropy))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_proba_entropy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
